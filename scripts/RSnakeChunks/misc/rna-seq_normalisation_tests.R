
## ---- Define the main_parameters -----------------------------------------------------

## Define main parameters to generate this report
dir.main <- "~/ko-rna-seq/" ## Main directory
setwd(dir.main)
message("\tMain directory: ", dir.main)

## Define YAML configuration file
configFile <- "metadata/config_RNA-seq.yml"
message("\tConfiguration file: ", configFile)

## Prefix for the count table
count.prefix <- "bowtie2_featureCounts_all"
message("\tPrefix for the count table: ", count.prefix)

## Relative path of the main dir starting from the Rmd file
dir.base <- ".."
message("\tBase directory: ", dir.base)

## ---- knitr_setup, include=FALSE,  eval=TRUE, echo=FALSE, warning=FALSE----

quick.test <- FALSE ## For debug

## I guess these options are not required when running R script without Rmd
knitr::opts_chunk$set(
  fig.path = "figures/",
  echo = FALSE,
  eval = TRUE,
  cache = FALSE,
  message = FALSE,
  warning = FALSE)

## Load required libraries
required.libraries <- c("knitr",
                        "yaml",
                        "pander",
                        # "xlsx",
                        "ascii",
                        "xtable",
                        "gplots",
                        "RColorBrewer",
                        "devtools"#,
                        #                        "stats4bioinfo" ## Generic library from Jacques van Helden
)
for (lib in required.libraries) {
  message("\tRequired CRAN library\t", lib)
  if (!require(lib, character.only = TRUE)) {
    install.packages(lib)
    library(lib, character.only = TRUE)
  }
}

# library(gplots, warn.conflicts = FALSE, quietly=TRUE) ## Required for heatmaps.2
#library(RColorBrewer, warn.conflicts = FALSE, quietly=TRUE)

required.bioconductor <- c(
  "edgeR",
  "DESeq2",
  "limma",
  #  "SARTools", ## for SERE coefficient
  "GenomicFeatures")

for (lib in required.bioconductor) {
  message("\tRequired BioConductor library\t", lib)
  if (!require(lib, character.only = TRUE)) {
    ## try http:// if https:// URLs are not supported
    source("https://bioconductor.org/biocLite.R")
    biocLite(lib)
  }
  if (!require(lib, character.only = TRUE)) {
    stop("Missing library: ", lib, " could not be installed")
  }
}


## ## Install SARTools if required
## message("\tRequired devtools library\t", "SARTools")
## if (!require("SARTools")) {
##   library(devtools)
##   install_github("PF2-pasteur-fr/SARTools", build_vignettes = TRUE)
## }

## ---- Detine in/outfiles ----

## Prepare a list of input and output files.
## This will later serve to generate a report.
infiles <- vector()   ## Input files
outfiles <- vector()  ## For tab-separated value files
figfiles <- vector()  ## Store figures

## ---- Check SnakeChunks directory ----------------------------------------------------

## ---- Load configuration file (YAML-formatted) ----
if (!exists("configFile")) {
  ## The prompt does not seem to work with the Rmd documents
  #   message("Choose the parameter file")
  #   parameter.file <- file.choose()
  stop("This report requires to specify a variable named configFile, containing the path to an YAML-formatted file describing the parameters for this analysis.")
}
parameters <- yaml.load_file(configFile)
message("\tLoaded parameters from file ", configFile)


if (is.null(parameters$dir$snakechunks)) {
  stop("The SnakeChunks directory should be defined in the config file: ", configFile)
}
dir.SnakeChunks <- file.path(dir.main, parameters$dir$snakechunks)
message("\tSnakeChunks directory:\t", dir.SnakeChunks)




## ----  Load R functions --------------------

## NOTE: if the RSnakechunks package has been compiled on this machine
## the functions will be loaded with library('RSnakeChunks'). However
## for the ime being we do not assume that RSnakeChunks has been
## compiled with SnakeChunks installation, so we souce all the files
## containing R functions.
R.dir <- file.path(dir.SnakeChunks, "scripts/RSnakeChunks/R")
R.files <- list.files(R.dir)
for (f in R.files) {
  message("\tLoading R file ", f)
  source(file.path(R.dir, f))
}

## R markdown (Rmd) directory
if (is.null(parameters$dir$Rmd)) {
  stop("The Rmd directory should be defined in the config file: ", configFile)
}
dir.Rmd <- parameters$dir$Rmd
message("\tDirectory for the Rmd report: ", dir.Rmd)
dir.create(dir.Rmd, showWarnings = FALSE, recursive = TRUE)
opts_knit$set(base.dir = dir.Rmd) ## Set the working directory for knitr (generating HTML and pdf reports)
# setwd(dir.Rmd) ## Set the working directory for the console


## R markdown (Rmd) directory
if (is.null(parameters$dir$figures)) {
  stop("The figures directory should be defined in the config file: ", configFile)
}
dir.figures <- parameters$dir$figures
message("\tDirectory for the generic figures: ", dir.figures)
dir.create(dir.figures, showWarnings = FALSE, recursive = TRUE)

## Directory to store differential expression results
dir.DEG <- parameters$dir$diffexpr
message("\tDirectory for differential expresion: ", dir.DEG)
dir.create(dir.DEG, showWarnings = FALSE, recursive = TRUE)

## Directory to export result files in tba-separated value (tsv) format
dir.tsv <- file.path(dir.DEG, "tsv_files")
message("\tDirectory to export TSV files:\t", dir.tsv)
dir.create(dir.tsv, showWarnings = FALSE, recursive = TRUE)

## ----default_parameters--------------------------------------------------
## In this chunk, we define a set of default parameters for the display and the analysis. These parameters can be modified but it is not necessary to adapt them to each project.
if ((!exists("verbosity")) || (is.null(verbosity))) {
  verbosity <- 1
}
if (!exists("export.excel.files")) {
  export.excel.files <- FALSE
}

## Color palette for heatmaps. I like this Red-Blue palette because
## - it suggests a subjective feeling of warm (high correlation)/cold (low correlation)
## - it can be seen by people suffering from redâ€“green color blindness.
if (!exists("cols.heatmap")) {
  cols.heatmap <- rev(colorRampPalette(brewer.pal(9,"RdBu"))(100))
}

## A trick: to enable log-scaled plots for 0 values, I add an epsilon increment
if (is.null(parameters$DEG$epsilon)) {
  parameters$DEG$epsilon <- 0.1 # passed to file parameters.R, 2017-03-15
}
epsilon <- parameters$DEG$epsilon

## Default method for the selection of the final list of DEG
if (is.null(parameters$DEG$selection_criterion)) {
  parameters$DEG$selection_criterion <- "DESeq2"
}
DEG.selection.criterion <- parameters$DEG$selection_criterion

## Sample description file
if (is.null(parameters$metadata$samples)) {
  stop("The sample file must be defined in the metadata seection of the yaml config file: ", configFile)
} else {
  infiles["sample descriptions"] <- parameters$metadata$samples
}

## Design file
if (is.null(parameters$metadata$design)) {
  stop("The design file must be defined in the metadata seection of the yaml config file: ", configFile)
} else {
  infiles["design"] <- parameters$metadata$design
}

## Count table
infiles["counts"] <- file.path(
  parameters$dir$diffexpr,
  paste(sep = "", count.prefix, ".tsv"))
all.counts.path <- file.path(dir.main, infiles["counts"])
if (!file.exists(all.counts.path)) {
  stop("Feature count table does not exist: ", all.counts.path)
} else {
  message("\tFeature count table: ", all.counts.path)
}



## ----threshold_table-----------------------------------------------------
if (is.null(parameters$DEG$thresholds)) {
  message("\tDEG thresholds were not defined in config file -> using default values")
  if (is.null(parameters$DEG)) {
    parameters$DEG <- list()
  }
  parameters$DEG$thresholds <- list(
    min.count = 1,
    mean.count = 5,
    padj = 0.05,
    FC = 1.2)
}
thresholds <- parameters$DEG$thresholds

## Print the threshold tables
## NOTE: I should evaluate what I do with the kable calls
kable(t(as.data.frame(thresholds)), col.names = "Threshold",
      caption = "Thresholds for the selection of differentially expressed genes. ")

outfiles["threshold"] <- file.path(dir.tsv, "thresholds.tsv")
write.table(x = t(as.data.frame(thresholds)),
            file = outfiles["threshold"],
            sep = "\t", row.names = TRUE, col.names = FALSE)
# list.files(dir.tsv)
# system(paste("open", dir.tsv))

## ----read_samples--------------------------------------------------------
#setwd(dir.main) ## !!!!! I don't understand why I have to reset the working directory at each chunk

## Read the sample description file, which indicates the
## condition associated to each sample ID.
message("Reading sample description file: ", infiles["sample descriptions"])
sample.desc <- read.delim(
  file.path(dir.main, infiles["sample descriptions"]), sep = "\t",
  comment = ";", header = TRUE, row.names = 1)
sample.ids <- row.names(sample.desc)
message("\tNb of samples = ", length(sample.ids))

## Experimental conditions
sample.conditions <- as.vector(sample.desc[,1]) ## Condition associated to each sample
names(sample.conditions) <- sample.ids
# print(sample.conditions)

## Build sample labels by concatenating their ID and condition
sample.desc$label <- paste(sep = "_", sample.ids, sample.conditions)

## Define a specific color for each distinct condition
conditions <- unique(sample.conditions) ## Set of distinct conditions
cols.conditions <- brewer.pal(max(3, length(conditions)),"Dark2")[1:length(conditions)]
names(cols.conditions) <- conditions
# print(cols.conditions)
message("\tNb of conditions = ", length(conditions))
message("\tConditions = ", paste(collapse = ", ", conditions))

## Define a color per sample according to its condition
sample.desc$color <- cols.conditions[sample.conditions]
# names(cols.samples) <- sample.ids
# print(cols.samples)

## Print the sapmple descriptons
kable(sample.desc, caption = "Sample description table")


## ----read_design, warning=FALSE------------------------------------------
# setwd(dir.main) ## !!!!! I don't understand why I have to reset the working directory at each chunk

## Read the design file, which indicates the anlayses to be done.
## Each row specifies one differential expression analysis, which
## consists in comparing two conditions.
message("Reading design file: ", infiles["design"])
design <- read.delim(file.path(dir.main, infiles["design"]), sep = "\t",
                     comment = ";", header = T, row.names = NULL)
message("\tDesign file contains ", nrow(design), " comparisons. ")
comparison.summary <- design ## Initialize a summary table for each DEG analysis
comparison.summary$prefixes <- paste(sep = "_", design[,1], "vs", design[,2])

## Print out the design table (pairs of conditions to be compared)
kable(comparison.summary,
      row.names = TRUE,
      caption = "**Design**. Each row describes one comparison between two conditions.")



## ----load_count_table----------------------------------------------------
message("Loading count table: ", all.counts.path)
ori.counts <- read.delim(all.counts.path, row.names = 1, sep = "\t")
# names(ori.counts)
# dim(ori.counts)
# View(ori.counts)

## Filter out the rows corresponding to non-assigned counts,
## e.g. __not_aligned, __ambiguous, __too_low_qAual, __not_aligned
not.feature <- grep(rownames(ori.counts), pattern = "^__")
if (length(not.feature) > 0) {
  all.counts <- ori.counts[-not.feature,]
} else {
  all.counts <- ori.counts
}
# dim(all.counts)

## Just for quick test and  debug: select a random subset of features
if (quick.test) {
  all.counts <- all.counts[sample(x = 1:nrow(all.counts), size = 1000, replace = FALSE),]
}
message("\tLoaded counts: ",
        nrow(all.counts), " features x ",
        ncol(all.counts), " samples")

## Check that the header of all.counts match the sample IDs
ids.not.found <- setdiff(sample.ids, names(all.counts)) ## Identify sample IDs with no column in the count table
if (length(ids.not.found) == length(sample.ids)) {
  colnames(all.counts) <- sample.ids
  ids.not.found <- setdiff(sample.ids, names(all.counts)) ## Identify
} else if (length(ids.not.found) > 0) {
  stop(length(ids.not.found), " missing columns in count table\t", all.counts.path,
       "\n\tMissing columns: ", paste(collapse = "; ", ids.not.found))
}

## Restrict the count table to the sample IDs found in the sample description file
all.counts <- all.counts[, sample.ids]
# names(all.counts)
# dim(all.counts)

##---- Feature filtering ----

## Load list of black-listed features
if (is.null(parameters$DEG$blacklist)) {
  black.listed.features <- NULL
} else {
  infiles["black_list"] <- parameters$DEG$blacklist
  black.list.table <- read.table(
    infiles["black_list"], sep = "\t",
    comment.char = "#",
    header = FALSE)
  black.listed.features <- as.vector(black.list.table[,1])
  black.listed.not.found <- setdiff(black.listed.features, row.names(all.counts))
  if (length(black.listed.not.found) > 0) {
    message("Warning: some IDs of the black list do not correspond to features of the count table")
    message("\tNot found IDs", paste(collapse = ", ", head(black.listed.not.found)))
  }
  black.listed.features <- intersect(black.listed.features, row.names(all.counts))
  message("Discarding ", length(black.listed.features), " black-listed features\t", parameters$DEG$blacklist)
}

## ---- Filter out features according to various user-specified criteria ----
## (parameters defined in the config files
filtered.counts <- FilterCountTable(
  counts = all.counts,
  na.omit = TRUE,
  min.count = thresholds$min.count,
  mean.count = thresholds$mean.count,
  mean.per.condition = threshold$mean.per.condition,
  black.list = black.listed.features)

##---- Log-transform non-normalized counts ----

## Add an epsilon to 0 values only, in order to enable log-transform and display on logarithmic axes.
message("\tTreating zero-values by adding epsilon =", epsilon)
filtered.counts.epsilon <- filtered.counts
filtered.counts.epsilon[filtered.counts == 0] <- epsilon

## Log-transformed data for some plots.
message("\tComputing log-transformed values")
filtered.counts.log10 <- log10(filtered.counts.epsilon)
filtered.counts.log2 <- log2(filtered.counts.epsilon)


## ----sample_statistics---------------------------------------------------

################################################################
## Compute sample-wise statistics on mapped counts
################################################################
message("Computing sample-wise statistics on non-filtered counts")
#stats.per.sample <- calc.stats.per.sample(sample.desc, all.counts)
# View(stats.per.sample)
stats.per.sample.all <- cbind(sample.desc, RowStats(all.counts))
stats.per.sample.all$Mreads <- stats.per.sample.all$sum / 1e6
# View(stats.per.sample.all.all)
outfiles["stats_per_sample_all_features"] <- file.path(dir.tsv, "stats_per_sample_all_features.tsv")
message("\t", outfiles["stats_per_sample_all_features"])
write.table(x = stats.per.sample.all, file = outfiles["stats_per_sample_all_features"], quote = FALSE, sep = "\t", row.names = TRUE, col.names = NA)

## Compute statistics ommitting zero values
message("Computing sample-wise statistics for non-zero counts")
all.counts.nozero <- all.counts
all.counts.nozero[all.counts.nozero == 0] <- NA
stats.per.sample.nozero <- cbind(sample.desc, RowStats(all.counts.nozero))
stats.per.sample.nozero$Mreads <- stats.per.sample.nozero$sum / 1e6
outfiles["stats_per_sample_no-zero"] <- file.path(dir.tsv, "stats_per_sample_no-zero.tsv")
message("\t", outfiles["stats_per_sample_no-zero"])
write.table(x = stats.per.sample.nozero, file = outfiles["stats_per_sample_no-zero"], quote = FALSE, sep = "\t", row.names = TRUE, col.names = NA)
# names(stats.per.sample)
# View(stats.per.sample.nozero)

## Compute statistics ommitting zero values
message("Computing sample-wise statistics for filtered counts")
stats.per.sample.filtered <- cbind(sample.desc, RowStats(filtered.counts))
stats.per.sample.filtered$Mreads <- stats.per.sample.filtered$sum / 1e6
# names(stats.per.sample)
# View(stats.per.sample.nozero)

################################################################
## Compute the counts per million reads
################################################################
message("Computing normalized values with edgeR::cpm")
## Note: the default normalization criterion (scaling by libbrary sum)
## is questionable because it is stronly sensitive to outliers
## (very highly expressed genes).  A more robust normalisation criterion
## is to use the 75th percentile, or the median. We use the median, somewhat arbitrarily,
## beause it gives a nice alignment on the boxplots.
stdcounts.libsum <- cpm(filtered.counts.epsilon)    ## Counts per million reads, normalised by library sum
stdcounts.perc75 <- cpm(filtered.counts.epsilon, lib.size = stats.per.sample$perc75)    ## Counts per million reads, normalised by 75th percentile
stdcounts.perc95 <- cpm(filtered.counts.epsilon, lib.size = stats.per.sample.filtered$perc95)    ## Counts per million reads, normalised by 95th percentile
stdcounts.median <- cpm(filtered.counts.epsilon, lib.size = stats.per.sample.filtered$median)    ## Counts per million reads, normalised by sample-wise median count

## Chose one of the standardization methods to get
#stdcounts <- stdcounts.median ## Choose one normalization factor for the stdcounts used below
stdcounts <- stdcounts.perc75 ## Choose one normalization factor for the stdcounts used below
stdcounts.log10 <- log10(stdcounts) ## Log-10 transformed stdcounts, xwith the epsilon for 0 counts
stdcounts.log2 <- log2(stdcounts) ## Log-10 transformed stdcounts, with the epsilon for 0 counts

## Export normalized counts (in log2-transformed counts per million reads)
outfiles["stdcounts"] <- paste(sep = "", count.prefix, "_stdcounts.tsv")
message("\tExporting standardized counts: ", outfiles["stdcounts"])
write.table(x = stdcounts, row.names = TRUE, col.names = NA,
            file = file.path(dir.main, outfiles["stdcounts"]), sep = "\t", quote = FALSE)

outfiles["log2stdcounts"] <- paste(sep = "", count.prefix, "_stdcounts_log2.tsv")
message("\tExporting log2-transformed standardized counts: ", outfiles["log2stdcounts"])
write.table(x = stdcounts.log2, row.names = TRUE, col.names = NA,
            file = outfiles["log2stdcounts"], sep = "\t", quote = FALSE)


# ## Detect outliers, i.e. genes with a very high number of reads (hundreds of thousands), most of which result from problems with ribodepletion.
# if (is.null(thresholds["max.log10.cpm"])) {
#   outlier.threshold <- 8.5 ## Somewhat arbitrary threshold to discard
# } else {
#   outlier.threshold <- thresholds["max.log10.cpm"]
# }
# outliers <- (apply(stdcounts.log10, 1, max) > outlier.threshold)
# message("\tDetected ", sum(outliers), " outliers with log10(stdcounts) higher than ", outlier.threshold)
# rownames(stdcounts.log10[outliers,])
# sum(outliers)

## Compute Trimmed Means of M Values (TMM): TO BE DONE
stats.per.sample.filtered$cpm.mean <- apply(stdcounts, 2, mean)
stats.per.sample.filtered$log2.cpm.mean <- apply(stdcounts.log2, 2, mean)
stats.per.sample.filtered$log10.cpm.mean <- apply(stdcounts.log10, 2, mean)

################################################################
## Export stats per sample
#
# names(stats.per.sample)
# head(stats.per.sample)
outfiles["stats_per_sample_filtered_features"] <- file.path(dir.tsv, "stats_per_sample_filtered_features.tsv")
message("\t", "Exporting stats per sample for filtered features")
message("\t", outfiles["stats_per_sample_filtered_features"])
write.table(x = stats.per.sample.nozero, file = outfiles["stats_per_sample_filtered_features"], quote = FALSE, sep = "\t", row.names = TRUE, col.names = NA)
# sample.summary.file <- paste(sep = "", count.prefix, "_summary_per_sample.tsv")
# sample.summary.file.path <- file.path(dir.main, paste(sep = "", count.prefix, "_summary_per_sample.tsv"))
# message("\tExporting stats per sample\t", sample.summary.file.path)
# write.table(x = stats.per.sample,
#             row.names = TRUE, col.names = NA,
#             file = sample.summary.file.path, sep = "\t", quote = FALSE)
# sample.summary.file.xlsx <- paste(sep = "", count.prefix, "_summary_per_sample.xlsx")
# if (export.excel.files) {
#   message(paste(sep = "", "\tSample summary file: ", sample.summary.file.xlsx))
#   write.xlsx(x = stats.per.sample, row.names = TRUE, col.names=TRUE,
#              file = file.path(dir.main, sample.summary.file.xlsx))
# }


## ----print_sample_stats--------------------------------------------------
## Statistics per sample
stats.per.sample.to.print <- c("Mreads",
                               "sum",
                               "min",
                               "zeros",
                               "non.null",
                               "perc05",
                               "Q1",
                               "mean",
                               "median",
                               "Q3",
                               "perc95",
                               "max",
                               "max.sum.ratio",
                               "median.mean.ratio",
                               "fract.below.mean")
# setdiff(stats.per.sample.to.print, names(stats.per.sample))

kable(stats.per.sample[stats.per.sample.to.print], digits = 2,
      format.args = list(big.mark = ",", decimal.mark = "."),
      caption = "Sample-wise statistics (zeros included)")

kable(stats.per.sample.nozero[stats.per.sample.to.print], digits = 2,
      format.args = list(big.mark = ",", decimal.mark = "."),
      caption = "Sample-wise statistics (zeros excluded)")


## ----library_sizes_barplot, fig.width=6, fig.height=6, fig.cap="**Barplot of assigned reads per sample. ** Bars indicate the sum of read counts assigned to features (genes) per sample (library)."----
par.ori <- par(no.readonly = TRUE) # Store original parameters
# par(c("mar", "mai"))
# libsize.barplot(
#   stats.per.sample,
#   plot.file = NULL,
#   main = "Assigned reads per sample (libsum)")
# par(par.ori) # Restore original parameters
#
figfiles["libsize_barplot_all_features"] <- file.path(dir.figures, "libsize_barplot_all_features.pdf")
message("\tLibrary size barplot for all features\t", figfiles["libsize_barplot_all_features"])
pdf(file = figfiles["libsize_barplot_all_features"], width = 8, height = 8)
LibsizeBarplot(counts = all.counts, sample.labels = sample.desc$label, sample.colors = sample.desc$color, main = "All features")
silence <- dev.off(); rm(silence)

figfiles["libsize_barplot_filtered_features"] <- file.path(dir.figures, "libsize_barplot_filtered_features.pdf")
message("\tLibrary size barplot after filtering\t", figfiles["libsize_barplot_filtered_features"])
pdf(file = figfiles["libsize_barplot_filtered_features"], width = 8, height = 8)
LibsizeBarplot(counts = filtered.counts, sample.labels = sample.desc$label, sample.colors = sample.desc$color, main = "After filtering")
silence <- dev.off(); rm(silence)


## ----normalisation-------------------------------------------------------
norm.methods <- c("none", "mean", "median", "percentile", "TMM", "DESeq2")
norm.comparison <- NormalizeCountTable(
  counts = filtered.counts, class.labels = sample.conditions, nozero = TRUE,
  method = norm.methods, percentile = 75, log2 = FALSE, epsilon = 0.1, detailed.sample.stats = TRUE,
  verbose = 2)
#names(norm.comparison)

## ----size_factors, fig.width=8, fig.height=8, fig.cap="Sample size factors for different normalisation methods. "----
# m <- "median"
size.factors <- data.frame(matrix(nrow = length(sample.ids), ncol = length(norm.methods)))
colnames(size.factors) <- norm.comparison$method.name
rownames(size.factors) <- sample.ids
for (m in norm.comparison$method.name) {
  size.factors[,m] <- norm.comparison[[m]]$size.factor
}

figfiles["size_factors"] <- file.path(dir.figures, "norm_size_factor_comparison.pdf")
pdf(file = figfiles["size_factors"], width = 8, height = 8)
plot(size.factors, main = "Sample size factors", col = sample.desc$color)
silence <- dev.off(); rm(silence)

## ----differential_expression_analysis, fig.width=8, fig.height=12--------
# setwd(dir.main) ## !!!!! I don't understand why I have to reset the working directory at each chunk

i <- 1

for (i in 1:nrow(design)) {


  prefix <- list() ## list for output file prefixes

  deg.results <- list()

  ## Identify samples for the first condition
  cond1 <- as.vector(design[i,1])  ## First condition for the current comparison
  samples1 <- sample.ids[sample.conditions == cond1]
  if (length(samples1) < 2) {
    stop(paste("Cannot perform differential analysis. The count table contains less than 2 samples for condition", cond1))
  }

  ## Identify samples for the second condition
  cond2 <- as.vector(design[i,2])  ## Second condition for the current comparison
  samples2 <- sample.ids[sample.conditions == cond2]
  if (length(samples2) < 2) {
    stop(paste("Cannot perform differential analysis. The count table contains less than 2 samples for condition", cond2))
  }

  #  stop("HELLO", "\tprefix = ", prefix)

  message("\tDifferential analysis\t", i , "/", nrow(design), "\t", cond1, " vs ", cond2)

  ## Create a specific result directory for this differential analysis
  comparison.prefix <- comparison.summary$prefixes[i]
  dir.analysis <- file.path(dir.DEG, paste(sep = "", comparison.prefix))
  comparison.summary[i, "dir.analysis"] <- dir.analysis
  dir.create(path = file.path(dir.main, dir.analysis), showWarnings = FALSE, recursive = TRUE)
  dir.figures <- file.path(dir.analysis, "figures")
  comparison.summary[i, "dir.figures"] <- dir.figures
  dir.create(path = file.path(dir.main, dir.figures), showWarnings = FALSE, recursive = TRUE)
  prefix["comparison_file"] <- file.path(dir.analysis, comparison.prefix)
  prefix["comparison_figure"] <- file.path(
    dir.figures,
    paste(sep = "", comparison.prefix))
  #    paste(sep = "", comparison.prefix, "_",  suffix.deg))


  ## Select counts for the samples belonging to the two conditions
  current.samples <- c(samples1, samples2)
  current.counts <- data.frame(filtered.counts[,current.samples])
  # dim(current.counts)  ## For test
  # names(current.counts)

  if (sum(!names(current.counts) %in% sample.ids) > 0) {
    stop("Count table contains column names without ID in sample description file.")
  }

  ## Define conditions and labels for the samples of the current analysis
  current.conditions <- sample.conditions[current.samples]
  current.labels <- paste(current.conditions, names(current.counts), sep = "_")

  result.table <- init.deg.table(stdcounts, samples1, samples2, stats = FALSE)
  # View(result.table)
  # dim(result.table)

  ################################################################
  ## DESeq2 analysis
  ################################################################
  message("\tDESeq2 analysis\t", comparison.prefix)
  deseq2.result <- deseq2.analysis(
    counts = current.counts,
    condition = current.conditions,
    comparison.prefix = comparison.prefix,
    ref.condition = cond2,
    title = comparison.prefix,
    dir.figures = file.path(dir.main, dir.figures))
  deg.results[["DESeq2"]] <- deseq2.result
  # names(deg.results[["DESeq2"]])
  #  attributes(deg.results[["DESeq2"]]$dds)
  # View(deg.results[["DESeq2"]]$result.table)

  #  head(rownames(deseq2.result$result.table))
  # head(rownames(result.table))
  # x <- rownames(result.table)
  # y <- rownames(deseq2.result$result.table[row.names(result.table),])
  # sum(x != y)
  # names(result.table)
  result.table <- cbind(
    result.table,
    "DESeq2" = deseq2.result$result.table[row.names(result.table),])
  # names(result.table)
  # dim(deseq2.result$result.table)
  # dim(deseq2.result$result.table)
  # names(deseq2.result$result.table)
  # View(deseq2.result$result.table)
  # View(result.table)


  ## Save the completed DESeq2 result table
  deseq2.result.file <- paste(sep = "_", prefix["comparison_file"], "DESeq2")
  comparison.summary[i,"deseq2"] <- paste(sep = ".", deseq2.result.file, "tsv")
  message("\tExporting DESeq2 result table (tab): ", deseq2.result.file, ".tsv")
  write.table(
    x = deseq2.result$result.table, row.name    = FALSE,
    file = file.path(dir.main, paste(sep = ".", deseq2.result.file, "tsv")),
    sep = "\t", quote = FALSE)

  ################################################################
  ## edgeR analysis
  ################################################################
  # norm.method <- "TMM" ## For quick test and debugging
  edgeR.norm.methods <- c("TMM","RLE","upperquartile","none")
  for (norm.method in edgeR.norm.methods) {

    edgeR.prefix <- paste(sep = "_", "edgeR", norm.method)

    edger.result <- edger.analysis(
      counts = current.counts,
      condition = current.conditions,
      ref.condition = cond2,
      comparison.prefix = comparison.prefix,
      title = comparison.prefix,
      norm.method = norm.method,
      dir.figures = file.path(dir.main, dir.figures))
    deg.results[[edgeR.prefix]] <- edger.result

    ## A tricky way to add edgeR with normalisation in column names
    edger.to.bind <- edger.result$result.table[row.names(result.table),]
    colnames(edger.to.bind) <- paste(sep = "_", edgeR.prefix, colnames(edger.to.bind))
    # names(edger.to.bind)
    # View(x)
    # x <- rownames(result.table)
    # y <- rownames(edger.to.bind)
    # sum(x != y)
    # names (result.table)
    result.table <- cbind(
      result.table,
      edger.to.bind)
    # names (result.table)


    ## Export edgeR result table
    edger.result.file <- paste(sep = "_", prefix["comparison_file"], edgeR.prefix)
    comparison.summary[i,"edger"] <- paste(sep = ".", edger.result.file, "tsv")
    message("\tExporting edgeR result table (tab): ", edger.result.file, ".tsv")
    write.table(x = edger.result$result.table,
                file = file.path(dir.main, paste(sep = ".", edger.result.file, "tsv")),
                row.names = FALSE,
                sep = "\t", quote = FALSE)
  }

  ## Export full result table (DESeq2 + edgeR with different normalisation methods)
  ## in a tab-separated values (tsv) file
  result.file <- paste(sep = "",
                       prefix["comparison_file"],
                       "_diffexpr_DESeq2_and_edgeR")
  # comparison.summary[i,"result.table"] <- paste(sep=".", result.file, "tsv")
  verbose(paste(sep = "", "\tExporting result table (tsv): ", result.file, ".tsv"), 1)
  write.table(x = result.table, row.names = FALSE,
              file = file.path(dir.main, paste(sep="", result.file, ".tsv")), sep = "\t", quote = FALSE)


  ## Collect results by output statistics
  deg.compa <- list()
  feature.ids <- row.names(current.counts)
  stats.to.collect <- c("padj", "FC", "log2FC")
  for (stat in stats.to.collect) {
    message("Collecting ", stat, " from alternative DEG results. ")
    deg.compa[[stat]] <- data.frame(
      matrix(nrow = nrow(current.counts),
             ncol = length(names(deg.results))))
    colnames(deg.compa[[stat]]) <- names(deg.results)
    rownames(deg.compa[[stat]]) <- feature.ids
    # deg.name <- "DESeq2"
    for (deg.name in names(deg.results)) {
      deg.compa[[stat]][feature.ids, deg.name] <-
        as.vector(deg.results[[deg.name]]$result.table[feature.ids,stat])
    }
    #    View(deg.compa[[stat]])
  }


  ## Define feature colors according to their level of expression (count means)
  feature.scores <- log2(apply(filtered.counts.epsilon, 1, median))

  # hist(feature.scores, breaks = 100)

  # View(deg.compa$padj)
  ## compare DESeq2 and edgeR normalisatio results

  ## ---- Plot comparing DEGs obtained with DESeq2 and edgeR ----

  ## Comparison between adjusted p-values
  prefix <- paste(sep = "", comparison.prefix, "_padj_comparisons")
  figfiles[prefix] <- file.path(dir.figures, paste(sep = "", prefix, ".pdf"))
  pdf(file = figfiles[prefix], width = 10, height = 10)
  plot(deg.compa$padj, log = "xy",
       #       col = FeatureColors(palette.type = "2col", scores = feature.scores),
       col = FeatureColors(palette.type = "dens",
                           x = deg.compa$padj[,1], y = deg.compa$padj[,2]),
       main = paste(sep = "", comparison.prefix, "\nAdjusted p-values"))
  silence <- dev.off(); rm(silence)

  prefix <- paste(sep = "", comparison.prefix, "_lof2FC_comparisons")
  figfiles[prefix] <- file.path(dir.figures, paste(sep = "", prefix, ".pdf"))
  pdf(file = figfiles[prefix], width = 10, height = 10)
  plot(deg.compa$log2FC,
       #       col = FeatureColors(palette.type = "2col", scores = feature.scores),
       col = FeatureColors(palette.type = "dens",
                           x = deg.compa$log2FC[,1], y = deg.compa$log2FC[,2]),
       main = paste(sep = "", comparison.prefix, "\nlog2(fold change)"))
  silence <- dev.off(); rm(silence)

  ## Draw Volcano plots
  # deg.name <- "DESeq2"
  # deg.name <- "edgeR_TMM"
  deg.names <- names(deg.results)

  nb.panels <- n2mfrow(length(deg.names))

  prefix <- paste(sep = "", comparison.prefix, "_volcano_plots")
  figfiles[prefix] <- file.path(dir.figures, paste(sep = "", prefix, ".pdf"))
  pdf(file = figfiles[prefix], width = 1 + nb.panels[1]*3, height = 1 + nb.panels[2]*3)
  par.ori <- par(no.readonly = TRUE)
  par(mfrow = nb.panels)
  # deg.name <- "DESeq2"

  for (deg.name in deg.names) {
    message("\tDrawing volcano plot\t", deg.name)
    deg.result.table <- deg.results[[deg.name]]$result.table
    table(deg.result.table$DEG)
    # head(deg.result.table)
    # names(deg.result.table)
    table(deg.result.table[c("padj_0.05", "FC_1.14", "DEG")])

    # plot(deg.result.table$log2FC,
    #      -log10(deg.result.table$padj), main = paste(comparison.prefix, deg.name))
    # # # View(deg.result.table)
    VolcanoPlot(multitest.table = deg.result.table,
                main = deg.name,
                effect.size.col = "log2FC",
                control.type = "padj",
                alpha = parameters$DEG$thresholds$padj,
                effect.threshold = parameters$DEG$thresholds$FC,
                legend.corner = "topleft")

  }
  par(mfrow = c(1,1))
  par(par.ori)
  silence <- dev.off(); rm(silence)
  ## system(paste("ls -ltr ", figfiles[prefix]))
}

## ----sessioninfo---------------------------------------------------------
## Print the complete list of libraries + versions used in this session
sessionInfo()

## ----job_done------------------------------------------------------------
message("Job done")

